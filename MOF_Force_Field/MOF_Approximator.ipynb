<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import MOFDataset\n",
    "import os.path as osp \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.curdir\n",
    "dataset = MOFDataset('FIGXAU_V2.csv','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOFDataset(49988)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from model import MOF_Net, run\n",
    "from MOLGCN import MOLGCN\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training Loss: 1975.3556 \t Validation Loss: 514.7431 \n",
      "Epoch 2 : Training Loss: 564.7342 \t Validation Loss: 470.6502 \n",
      "Epoch 3 : Training Loss: 502.4572 \t Validation Loss: 423.8463 \n",
      "Epoch 4 : Training Loss: 455.0027 \t Validation Loss: 422.1005 \n",
      "Epoch 5 : Training Loss: 402.6585 \t Validation Loss: 415.1606 \n",
      "Epoch 6 : Training Loss: 380.8641 \t Validation Loss: 415.1636 \n",
      "Epoch 7 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 8 : Training Loss: 380.8648 \t Validation Loss: 415.1636 \n",
      "Epoch 9 : Training Loss: 380.8714 \t Validation Loss: 415.1636 \n",
      "Epoch 10 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 11 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 12 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 13 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 14 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n",
      "Epoch 15 : Training Loss: 380.8642 \t Validation Loss: 415.1636 \n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "batch_size = 2\n",
    "one_tenth_length = int(len(dataset) * 0.1)\n",
    "\n",
    "train_dataset = dataset[:one_tenth_length * 8]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = dataset[one_tenth_length * 8 :]\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MOF_Net(5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = [] \n",
    "\n",
    "for epoch in range(100):\n",
    "    training_loss = run(train_loader,model,optimizer,loss_func,device,True)\n",
    "    val_loss = run(val_loader,\n",
    "                   model,\n",
    "                   optimizer,\n",
    "                   loss_func,\n",
    "                   device,\n",
    "                   False)\n",
    "    train_loss_list.append(training_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(\"Epoch {} : Training Loss: {:.4f} \\t Validation Loss: {:.4f} \".format(epoch + 1, training_loss, val_loss))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import MOFDataset\n",
    "import os.path as osp \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.curdir\n",
    "dataset = MOFDataset('FIGXAU_V2.csv','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOFDataset(49988)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from model import MOF_Net, run\n",
    "from MOLGCN import MOLGCN\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The target values are very skewed to a small range. \n",
    "#### About 90% of the data fits in the bin [-927.91 , -920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentials = dataset.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.5005)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(potentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 556.,  278.,  153.,  182.,  226.,  146.,  178.,  202.,  297.,\n",
       "         266.,  246.,  281.,  253.,  199.,  196.,  209.,  218.,  204.,\n",
       "         224.,  257.,  266.,  271.,  397.,  480.,  494.,  649.,  873.,\n",
       "        1222., 1228., 1387., 1767., 2350., 3276., 4778., 7246., 7899.,\n",
       "        4104., 1734., 1515., 3281.]),\n",
       " array([4.6326321e-07, 3.6620732e-02, 7.3241003e-02, 1.0986127e-01,\n",
       "        1.4648154e-01, 1.8310180e-01, 2.1972208e-01, 2.5634235e-01,\n",
       "        2.9296261e-01, 3.2958287e-01, 3.6620316e-01, 4.0282342e-01,\n",
       "        4.3944368e-01, 4.7606397e-01, 5.1268423e-01, 5.4930449e-01,\n",
       "        5.8592474e-01, 6.2254500e-01, 6.5916532e-01, 6.9578558e-01,\n",
       "        7.3240584e-01, 7.6902610e-01, 8.0564636e-01, 8.4226662e-01,\n",
       "        8.7888694e-01, 9.1550720e-01, 9.5212746e-01, 9.8874772e-01,\n",
       "        1.0253680e+00, 1.0619882e+00, 1.0986085e+00, 1.1352288e+00,\n",
       "        1.1718490e+00, 1.2084693e+00, 1.2450897e+00, 1.2817099e+00,\n",
       "        1.3183302e+00, 1.3549504e+00, 1.3915707e+00, 1.4281909e+00,\n",
       "        1.4648112e+00], dtype=float32),\n",
       " <BarContainer object of 40 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "potentials = (potentials - torch.mean(potentials)) / torch.std(potentials)\n",
    "potentials = torch.exp(-potentials)\n",
    "pt = potentials.numpy()\n",
    "plt.hist(pt, bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple log transformation is not particularly useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neither is Standard Scaling \n",
    "#### (Transformation such that the dataset mean = 0, and std_dev = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Exp after Standard Scaling generates a better spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Untransformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training Loss: 110266.694100 \t Validation Loss: 732.240458 \n",
      "Epoch 2 : Training Loss: 720.799882 \t Validation Loss: 780.968136 \n",
      "Epoch 3 : Training Loss: 708.477480 \t Validation Loss: 732.392009 \n",
      "Epoch 4 : Training Loss: 694.077594 \t Validation Loss: 713.312580 \n",
      "Epoch 5 : Training Loss: 689.168226 \t Validation Loss: 718.213595 \n",
      "Epoch 6 : Training Loss: 671.110715 \t Validation Loss: 706.132896 \n",
      "Epoch 7 : Training Loss: 673.813091 \t Validation Loss: 677.487498 \n",
      "Epoch 8 : Training Loss: 654.991332 \t Validation Loss: 675.285056 \n",
      "Epoch 9 : Training Loss: 632.852649 \t Validation Loss: 648.947041 \n",
      "Epoch 10 : Training Loss: 618.637104 \t Validation Loss: 656.358706 \n",
      "Epoch 11 : Training Loss: 609.313472 \t Validation Loss: 644.124590 \n",
      "Epoch 12 : Training Loss: 601.902360 \t Validation Loss: 683.570098 \n",
      "Epoch 13 : Training Loss: 599.782040 \t Validation Loss: 630.906302 \n",
      "Epoch 14 : Training Loss: 588.231610 \t Validation Loss: 657.792793 \n",
      "Epoch 15 : Training Loss: 584.177805 \t Validation Loss: 620.360989 \n",
      "Epoch 16 : Training Loss: 577.068468 \t Validation Loss: 647.948450 \n",
      "Epoch 17 : Training Loss: 576.142076 \t Validation Loss: 596.337073 \n",
      "Epoch 18 : Training Loss: 568.017261 \t Validation Loss: 614.120188 \n",
      "Epoch 19 : Training Loss: 568.298961 \t Validation Loss: 633.840453 \n",
      "Epoch 20 : Training Loss: 563.051017 \t Validation Loss: 630.731733 \n",
      "Epoch 21 : Training Loss: 558.471611 \t Validation Loss: 599.341386 \n",
      "Epoch 22 : Training Loss: 552.548457 \t Validation Loss: 603.994074 \n",
      "Epoch 23 : Training Loss: 545.175479 \t Validation Loss: 594.564183 \n",
      "Epoch 24 : Training Loss: 548.600797 \t Validation Loss: 593.485176 \n",
      "Epoch 25 : Training Loss: 543.060374 \t Validation Loss: 593.278680 \n",
      "Epoch 26 : Training Loss: 542.014104 \t Validation Loss: 591.654468 \n",
      "Epoch 27 : Training Loss: 531.633080 \t Validation Loss: 568.511146 \n",
      "Epoch 28 : Training Loss: 531.525435 \t Validation Loss: 600.458439 \n",
      "Epoch 29 : Training Loss: 531.584559 \t Validation Loss: 562.164673 \n",
      "Epoch 30 : Training Loss: 530.416908 \t Validation Loss: 545.274314 \n",
      "Epoch 31 : Training Loss: 519.465691 \t Validation Loss: 557.946479 \n",
      "Epoch 32 : Training Loss: 522.875948 \t Validation Loss: 605.460049 \n",
      "Epoch 33 : Training Loss: 518.599975 \t Validation Loss: 594.895291 \n",
      "Epoch 34 : Training Loss: 520.173403 \t Validation Loss: 560.349200 \n",
      "Epoch 35 : Training Loss: 515.218515 \t Validation Loss: 555.136699 \n",
      "Epoch 36 : Training Loss: 511.928880 \t Validation Loss: 603.728528 \n",
      "Epoch 37 : Training Loss: 507.041137 \t Validation Loss: 547.148000 \n",
      "Epoch 38 : Training Loss: 508.151503 \t Validation Loss: 538.663886 \n",
      "Epoch 39 : Training Loss: 508.943939 \t Validation Loss: 552.135482 \n",
      "Epoch 40 : Training Loss: 501.892393 \t Validation Loss: 524.447438 \n",
      "Epoch 41 : Training Loss: 504.912580 \t Validation Loss: 560.569535 \n",
      "Epoch 42 : Training Loss: 501.998779 \t Validation Loss: 555.831746 \n",
      "Epoch 43 : Training Loss: 497.662532 \t Validation Loss: 540.789300 \n",
      "Epoch 44 : Training Loss: 495.534433 \t Validation Loss: 526.173501 \n",
      "Epoch 45 : Training Loss: 493.116306 \t Validation Loss: 520.542747 \n",
      "Epoch 46 : Training Loss: 494.461239 \t Validation Loss: 535.882059 \n",
      "Epoch 47 : Training Loss: 495.751889 \t Validation Loss: 519.603862 \n",
      "Epoch 48 : Training Loss: 487.369245 \t Validation Loss: 521.054057 \n",
      "Epoch 49 : Training Loss: 489.735886 \t Validation Loss: 527.422985 \n",
      "Epoch 50 : Training Loss: 483.990365 \t Validation Loss: 542.062774 \n",
      "Epoch 51 : Training Loss: 486.280404 \t Validation Loss: 530.987678 \n",
      "Epoch 52 : Training Loss: 483.624104 \t Validation Loss: 515.954955 \n",
      "Epoch 53 : Training Loss: 480.697127 \t Validation Loss: 509.557285 \n",
      "Epoch 54 : Training Loss: 480.986662 \t Validation Loss: 521.281759 \n",
      "Epoch 55 : Training Loss: 478.572529 \t Validation Loss: 501.816424 \n",
      "Epoch 56 : Training Loss: 474.614500 \t Validation Loss: 508.891882 \n",
      "Epoch 57 : Training Loss: 474.326312 \t Validation Loss: 532.073376 \n",
      "Epoch 58 : Training Loss: 474.901485 \t Validation Loss: 502.477979 \n",
      "Epoch 59 : Training Loss: 472.435161 \t Validation Loss: 515.257728 \n",
      "Epoch 60 : Training Loss: 466.104691 \t Validation Loss: 512.806947 \n",
      "Epoch 61 : Training Loss: 466.908580 \t Validation Loss: 531.434982 \n",
      "Epoch 62 : Training Loss: 467.891335 \t Validation Loss: 502.032755 \n",
      "Epoch 63 : Training Loss: 466.239815 \t Validation Loss: 508.854825 \n",
      "Epoch 64 : Training Loss: 462.380947 \t Validation Loss: 516.200301 \n",
      "Epoch 65 : Training Loss: 464.168607 \t Validation Loss: 501.664076 \n",
      "Epoch 66 : Training Loss: 461.599294 \t Validation Loss: 492.751902 \n",
      "Epoch 67 : Training Loss: 460.265488 \t Validation Loss: 515.744030 \n",
      "Epoch 68 : Training Loss: 461.231300 \t Validation Loss: 506.112337 \n",
      "Epoch 69 : Training Loss: 454.937586 \t Validation Loss: 505.461430 \n",
      "Epoch 70 : Training Loss: 457.454876 \t Validation Loss: 483.844165 \n",
      "Epoch 71 : Training Loss: 454.608318 \t Validation Loss: 501.164879 \n",
      "Epoch 72 : Training Loss: 452.769585 \t Validation Loss: 500.898081 \n",
      "Epoch 73 : Training Loss: 456.052057 \t Validation Loss: 499.723542 \n",
      "Epoch 74 : Training Loss: 450.854369 \t Validation Loss: 503.583052 \n",
      "Epoch 75 : Training Loss: 451.309119 \t Validation Loss: 498.594783 \n",
      "Epoch 76 : Training Loss: 450.492095 \t Validation Loss: 491.183132 \n",
      "Epoch 77 : Training Loss: 448.666836 \t Validation Loss: 482.752363 \n",
      "Epoch 78 : Training Loss: 446.950604 \t Validation Loss: 481.039815 \n",
      "Epoch 79 : Training Loss: 450.104226 \t Validation Loss: 473.679065 \n",
      "Epoch 80 : Training Loss: 445.526981 \t Validation Loss: 487.034016 \n",
      "Epoch 81 : Training Loss: 443.551556 \t Validation Loss: 478.089698 \n",
      "Epoch 82 : Training Loss: 442.434252 \t Validation Loss: 476.991361 \n",
      "Epoch 83 : Training Loss: 439.728657 \t Validation Loss: 477.337613 \n",
      "Epoch 84 : Training Loss: 441.966852 \t Validation Loss: 488.175752 \n",
      "Epoch 85 : Training Loss: 441.234044 \t Validation Loss: 470.926791 \n",
      "Epoch 86 : Training Loss: 441.874648 \t Validation Loss: 483.449624 \n",
      "Epoch 87 : Training Loss: 441.431313 \t Validation Loss: 485.487794 \n",
      "Epoch 88 : Training Loss: 435.673576 \t Validation Loss: 467.151463 \n",
      "Epoch 89 : Training Loss: 437.617308 \t Validation Loss: 474.515864 \n",
      "Epoch 90 : Training Loss: 434.178926 \t Validation Loss: 480.150088 \n",
      "Epoch 91 : Training Loss: 435.316247 \t Validation Loss: 471.274279 \n",
      "Epoch 92 : Training Loss: 431.492879 \t Validation Loss: 470.271751 \n",
      "Epoch 93 : Training Loss: 433.683225 \t Validation Loss: 476.395734 \n",
      "Epoch 94 : Training Loss: 431.084861 \t Validation Loss: 461.685511 \n",
      "Epoch 95 : Training Loss: 431.495323 \t Validation Loss: 459.679597 \n",
      "Epoch 96 : Training Loss: 429.744028 \t Validation Loss: 463.730033 \n",
      "Epoch 97 : Training Loss: 427.686393 \t Validation Loss: 466.026698 \n",
      "Epoch 98 : Training Loss: 428.366881 \t Validation Loss: 456.733334 \n",
      "Epoch 99 : Training Loss: 429.691486 \t Validation Loss: 454.569729 \n",
      "Epoch 100 : Training Loss: 427.686889 \t Validation Loss: 463.097818 \n",
      "Epoch 101 : Training Loss: 426.759910 \t Validation Loss: 461.791105 \n",
      "Epoch 102 : Training Loss: 423.903650 \t Validation Loss: 449.489431 \n",
      "Epoch 103 : Training Loss: 425.923283 \t Validation Loss: 456.398314 \n",
      "Epoch 104 : Training Loss: 426.353742 \t Validation Loss: 453.592290 \n",
      "Epoch 105 : Training Loss: 424.598382 \t Validation Loss: 460.222055 \n",
      "Epoch 106 : Training Loss: 423.527928 \t Validation Loss: 462.701288 \n",
      "Epoch 107 : Training Loss: 422.348084 \t Validation Loss: 451.262218 \n",
      "Epoch 108 : Training Loss: 421.322338 \t Validation Loss: 446.084778 \n",
      "Epoch 109 : Training Loss: 419.740426 \t Validation Loss: 455.923592 \n",
      "Epoch 110 : Training Loss: 421.900843 \t Validation Loss: 460.443402 \n",
      "Epoch 111 : Training Loss: 419.832869 \t Validation Loss: 455.756375 \n",
      "Epoch 112 : Training Loss: 418.012420 \t Validation Loss: 451.024520 \n",
      "Epoch 113 : Training Loss: 417.988293 \t Validation Loss: 457.708174 \n",
      "Epoch 114 : Training Loss: 417.551753 \t Validation Loss: 452.435553 \n",
      "Epoch 115 : Training Loss: 417.939606 \t Validation Loss: 450.179609 \n",
      "Epoch 116 : Training Loss: 416.445223 \t Validation Loss: 450.532591 \n",
      "Epoch 117 : Training Loss: 416.396355 \t Validation Loss: 451.086282 \n",
      "Epoch 118 : Training Loss: 416.407648 \t Validation Loss: 444.235384 \n",
      "Epoch 119 : Training Loss: 416.399794 \t Validation Loss: 444.991264 \n",
      "Epoch 120 : Training Loss: 413.158918 \t Validation Loss: 449.805880 \n",
      "Epoch 121 : Training Loss: 416.616417 \t Validation Loss: 442.782451 \n",
      "Epoch 122 : Training Loss: 413.917057 \t Validation Loss: 444.042260 \n",
      "Epoch 123 : Training Loss: 414.016161 \t Validation Loss: 449.604025 \n",
      "Epoch 124 : Training Loss: 413.039502 \t Validation Loss: 438.287268 \n",
      "Epoch 125 : Training Loss: 412.560882 \t Validation Loss: 440.403872 \n",
      "Epoch 126 : Training Loss: 411.387234 \t Validation Loss: 446.731629 \n",
      "Epoch 127 : Training Loss: 410.691866 \t Validation Loss: 435.537001 \n",
      "Epoch 128 : Training Loss: 410.724651 \t Validation Loss: 443.828739 \n",
      "Epoch 129 : Training Loss: 409.289033 \t Validation Loss: 438.680353 \n",
      "Epoch 130 : Training Loss: 410.556029 \t Validation Loss: 443.147213 \n",
      "Epoch 131 : Training Loss: 409.368389 \t Validation Loss: 441.986299 \n",
      "Epoch 132 : Training Loss: 407.471375 \t Validation Loss: 436.815045 \n",
      "Epoch 133 : Training Loss: 410.178536 \t Validation Loss: 434.402517 \n",
      "Epoch 134 : Training Loss: 408.101876 \t Validation Loss: 443.486661 \n",
      "Epoch 135 : Training Loss: 408.768301 \t Validation Loss: 437.212468 \n",
      "Epoch 136 : Training Loss: 407.938728 \t Validation Loss: 436.213192 \n",
      "Epoch 137 : Training Loss: 408.606860 \t Validation Loss: 440.810665 \n",
      "Epoch 138 : Training Loss: 406.795915 \t Validation Loss: 442.049595 \n",
      "Epoch 139 : Training Loss: 406.017056 \t Validation Loss: 432.019359 \n",
      "Epoch 140 : Training Loss: 406.417068 \t Validation Loss: 434.417682 \n",
      "Epoch 141 : Training Loss: 406.408212 \t Validation Loss: 435.081015 \n",
      "Epoch 142 : Training Loss: 404.770489 \t Validation Loss: 433.819988 \n",
      "Epoch 143 : Training Loss: 403.491281 \t Validation Loss: 435.305478 \n",
      "Epoch 144 : Training Loss: 405.424777 \t Validation Loss: 432.549000 \n",
      "Epoch 145 : Training Loss: 403.404828 \t Validation Loss: 429.971070 \n",
      "Epoch 146 : Training Loss: 403.131259 \t Validation Loss: 427.907494 \n",
      "Epoch 147 : Training Loss: 403.612503 \t Validation Loss: 433.777232 \n",
      "Epoch 148 : Training Loss: 403.165047 \t Validation Loss: 430.055071 \n",
      "Epoch 149 : Training Loss: 405.108452 \t Validation Loss: 430.692801 \n",
      "Epoch 150 : Training Loss: 402.020722 \t Validation Loss: 425.091893 \n",
      "Epoch 151 : Training Loss: 402.148775 \t Validation Loss: 429.150998 \n",
      "Epoch 152 : Training Loss: 400.914607 \t Validation Loss: 433.716493 \n",
      "Epoch 153 : Training Loss: 402.141968 \t Validation Loss: 434.325121 \n",
      "Epoch 154 : Training Loss: 402.490539 \t Validation Loss: 432.794210 \n",
      "Epoch 155 : Training Loss: 400.921130 \t Validation Loss: 430.121964 \n",
      "Epoch 156 : Training Loss: 401.546582 \t Validation Loss: 428.479618 \n",
      "Epoch 157 : Training Loss: 400.947007 \t Validation Loss: 429.147266 \n",
      "Epoch 158 : Training Loss: 400.813826 \t Validation Loss: 428.014712 \n",
      "Epoch 159 : Training Loss: 400.639025 \t Validation Loss: 430.383091 \n",
      "Epoch 160 : Training Loss: 400.279730 \t Validation Loss: 429.465379 \n",
      "Epoch 161 : Training Loss: 399.520023 \t Validation Loss: 426.387329 \n",
      "Epoch 162 : Training Loss: 398.637998 \t Validation Loss: 430.400892 \n",
      "Epoch 163 : Training Loss: 398.849208 \t Validation Loss: 427.042447 \n",
      "Epoch 164 : Training Loss: 398.687307 \t Validation Loss: 428.456313 \n",
      "Epoch 165 : Training Loss: 398.909652 \t Validation Loss: 431.549412 \n",
      "Epoch 166 : Training Loss: 398.459772 \t Validation Loss: 429.967914 \n",
      "Epoch 167 : Training Loss: 398.076322 \t Validation Loss: 428.393551 \n",
      "Epoch 168 : Training Loss: 398.664012 \t Validation Loss: 433.305890 \n",
      "Epoch 169 : Training Loss: 398.514453 \t Validation Loss: 426.446381 \n",
      "Epoch 170 : Training Loss: 397.534338 \t Validation Loss: 427.406612 \n",
      "Epoch 171 : Training Loss: 396.557444 \t Validation Loss: 427.421324 \n",
      "Epoch 172 : Training Loss: 396.565874 \t Validation Loss: 424.819410 \n",
      "Epoch 173 : Training Loss: 396.606585 \t Validation Loss: 432.007277 \n",
      "Epoch 174 : Training Loss: 396.717201 \t Validation Loss: 431.976310 \n",
      "Epoch 175 : Training Loss: 397.114802 \t Validation Loss: 428.604632 \n",
      "Epoch 176 : Training Loss: 397.219352 \t Validation Loss: 429.289401 \n",
      "Epoch 177 : Training Loss: 396.257588 \t Validation Loss: 425.510218 \n",
      "Epoch 178 : Training Loss: 395.850971 \t Validation Loss: 428.673272 \n",
      "Epoch 179 : Training Loss: 395.349016 \t Validation Loss: 429.383504 \n",
      "Epoch 180 : Training Loss: 394.945305 \t Validation Loss: 428.970213 \n",
      "Epoch 181 : Training Loss: 395.675361 \t Validation Loss: 429.684870 \n",
      "Epoch 182 : Training Loss: 395.549370 \t Validation Loss: 427.379596 \n",
      "Epoch 183 : Training Loss: 394.020800 \t Validation Loss: 423.356388 \n",
      "Epoch 184 : Training Loss: 394.635895 \t Validation Loss: 428.546497 \n",
      "Epoch 185 : Training Loss: 394.727042 \t Validation Loss: 428.488461 \n",
      "Epoch 186 : Training Loss: 394.915296 \t Validation Loss: 429.224583 \n",
      "Epoch 187 : Training Loss: 394.295030 \t Validation Loss: 424.364678 \n",
      "Epoch 188 : Training Loss: 394.506433 \t Validation Loss: 424.550446 \n",
      "Epoch 189 : Training Loss: 394.653358 \t Validation Loss: 425.056571 \n",
      "Epoch 190 : Training Loss: 394.809246 \t Validation Loss: 425.177138 \n",
      "Epoch 191 : Training Loss: 393.227230 \t Validation Loss: 425.101551 \n",
      "Epoch 192 : Training Loss: 393.724695 \t Validation Loss: 428.021975 \n",
      "Epoch 193 : Training Loss: 392.889693 \t Validation Loss: 425.682760 \n",
      "Epoch 194 : Training Loss: 393.014170 \t Validation Loss: 427.040393 \n",
      "Epoch 195 : Training Loss: 394.436993 \t Validation Loss: 427.120987 \n",
      "Epoch 196 : Training Loss: 392.780331 \t Validation Loss: 424.493549 \n",
      "Epoch 197 : Training Loss: 393.387765 \t Validation Loss: 424.237745 \n",
      "Epoch 198 : Training Loss: 392.401091 \t Validation Loss: 425.038078 \n",
      "Epoch 199 : Training Loss: 392.972811 \t Validation Loss: 427.371402 \n",
      "Epoch 200 : Training Loss: 392.710068 \t Validation Loss: 423.617627 \n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "\n",
    "training_batch_size = 64\n",
    "validation_batch_size = 256\n",
    "\n",
    "one_tenth_length = int(len(dataset) * 0.1)\n",
    "\n",
    "train_dataset = dataset[:one_tenth_length * 8]\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_batch_size)\n",
    "\n",
    "val_dataset = dataset[one_tenth_length * 8 :]\n",
    "val_loader = DataLoader(val_dataset, batch_size = validation_batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MOF_Net(9).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = [] \n",
    "\n",
    "for epoch in range(200):\n",
    "    training_loss = run(train_loader,model,optimizer,loss_func,device,True)\n",
    "    val_loss = run(val_loader,\n",
    "                   model,\n",
    "                   optimizer,\n",
    "                   loss_func,\n",
    "                   device,\n",
    "                   False)\n",
    "    train_loss_list.append(training_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(\"Epoch {} : Training Loss: {:.6f} \\t Validation Loss: {:.6f} \".format(epoch + 1, training_loss, val_loss))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network outputs the same value as it gets stuck in a local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \n",
      " \t tensor([-920.2849, -920.2849, -920.2849, -920.2849, -920.2849, -920.2849,\n",
      "        -920.2849, -920.2849, -920.2849, -920.2849, -920.2849, -920.2849,\n",
      "        -920.2849, -920.2849, -920.2849, -920.2849], device='cuda:0')\n",
      "Actual: \n",
      " \t tensor([-927.8462, -924.0977, -927.7908, -920.2141, -927.8142, -925.0608,\n",
      "        -922.3284, -925.4791, -922.2180, -922.2498, -924.5257, -924.7504,\n",
      "        -920.4871, -927.9094, -925.5422, -925.6179], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "#         print(data)\n",
    "        y = model(data.to(device))\n",
    "        print(\"Predicted: \\n \\t\", y)\n",
    "        print(\"Actual: \\n \\t\", data.y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> parent of 8725cd7... Updated dataloader
